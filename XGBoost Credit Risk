# Assignment #1: Credit Risk Prediction with XGBoost

# 1. Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from xgboost import XGBClassifier

# 2. Load the dataset from GitHub (raw CSV format)
url = 'https://github.com/Safa1615/Dataset--loan/blob/main/bank-loan.csv?raw=true'
data = pd.read_csv(url, nrows=700)

# 3. Explore dataset structure
from IPython.display import display

print("\nðŸ“„ Columns:")
display(pd.DataFrame(data.columns, columns=['Column Names']))

print("\nðŸ“‹ First 5 rows:")
display(data.head())

print("\nðŸ“Š Summary statistics:")
display(data.describe().style.set_caption("Descriptive Statistics").format("{:.2f}"))

print("\nðŸ—ƒ Dtypes:")
display(pd.DataFrame(data.dtypes, columns=['Data Type']).style.set_caption("Feature Data Types"))

# 4. Visualize correlations between variables
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

# 5. Visualize distributions and pairwise relationships
sns.pairplot(data, vars=['age', 'employ', 'income', 'debtinc'], hue='default')
plt.suptitle("Pair Plot of Numeric Variables", y=1.02)
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(y='income', data=data)
plt.title("Box Plot of Income")
plt.ylabel("Income")
plt.show()

# 6. Handle missing values
missing = data.isnull().sum()
print("\nðŸš¨ Missing Values:")
display(missing[missing > 0].to_frame(name='Missing Count'))
data = data.dropna()

# 7. Feature matrix and target variable
X = data.drop('default', axis=1)
y = data['default']

# 8. Scale features to normalize data distribution
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 9. Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 10. Initialize and tune XGBoost model with hyperparameters
xgb = XGBClassifier(eval_metric='logloss', random_state=42)
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1]
}

# 11. Perform Grid Search for best hyperparameters
grid = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='accuracy')
grid.fit(X_train, y_train)
best_model = grid.best_estimator_

# 12. Make predictions
y_pred = best_model.predict(X_test)

# 13. Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print("\nâœ… Accuracy:", round(accuracy, 2))

print("\nðŸ“Š Confusion Matrix:")
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

print("\nðŸ“‘ Classification Report:")
report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()
display(report_df.style.set_caption("Classification Report").format("{:.2f}"))

# ===================================
# ðŸ§  Reflection & Commentary
# ===================================
# This project transitioned from a Random Forest to an XGBoost model to harness the power of gradient boosting.
# While Random Forest is simple and robust, XGBoost is elite software for tabular data, giving more control and precision.
# Using GridSearchCV made it easier to tune and maximize model performance.
# In a credit risk scenario, XGBoost's ability to reduce bias and variance makes it suitable for minimizing false negatives,
# which helps financial institutions avoid lending to high-risk borrowers.
# This upgrade from traditional methods represents the future of credit prediction through intelligent software.
